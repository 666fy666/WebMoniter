"""LLM å®¢æˆ·ç«¯ - æ”¯æŒä¸»æµåŽ‚å•† OpenAI å…¼å®¹ API"""

import json
import logging
from collections.abc import AsyncIterator
from typing import Any

from src.ai_assistant.config import get_ai_config

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ openaiï¼Œè‹¥æ— åˆ™å›žé€€åˆ° httpx
try:
    from openai import AsyncOpenAI, OpenAI

    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    import httpx

    HAS_HTTPX = True
except ImportError:
    HAS_HTTPX = False


def _create_client():
    """åˆ›å»º OpenAI å…¼å®¹å®¢æˆ·ç«¯ï¼ˆåŒæ­¥ï¼‰"""
    cfg = get_ai_config()
    base = cfg.get_api_base()
    key = cfg.get_api_key()
    if HAS_OPENAI:
        return OpenAI(api_key=key or "sk-placeholder", base_url=base if base else None)
    if HAS_HTTPX:
        return _HttpClient(base, key)
    raise RuntimeError("è¯·å®‰è£… openai æˆ– httpxï¼šuv sync")


def _create_async_client():
    """åˆ›å»º OpenAI å…¼å®¹å®¢æˆ·ç«¯ï¼ˆå¼‚æ­¥ï¼‰"""
    cfg = get_ai_config()
    base = cfg.get_api_base()
    key = cfg.get_api_key()
    if HAS_OPENAI:
        return AsyncOpenAI(api_key=key or "sk-placeholder", base_url=base if base else None)
    if HAS_HTTPX:
        return _AsyncHttpClient(base, key)
    raise RuntimeError("è¯·å®‰è£… openai æˆ– httpxï¼šuv sync")


class _HttpClient:
    """åŸºäºŽ httpx çš„åŒæ­¥ HTTP å®¢æˆ·ç«¯ï¼Œå…¼å®¹ OpenAI API æ ¼å¼"""

    def __init__(self, base_url: str, api_key: str):
        self.base_url = (base_url or "https://api.openai.com/v1").rstrip("/")
        self.api_key = api_key or "sk-placeholder"

    def chat_completions_create(self, **kwargs: Any) -> Any:
        url = f"{self.base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": kwargs.get("model", "gpt-4o-mini"),
            "messages": kwargs.get("messages", []),
            "stream": kwargs.get("stream", False),
        }
        if "temperature" in kwargs:
            payload["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            payload["max_tokens"] = kwargs["max_tokens"]
        with httpx.Client(timeout=60.0) as client:
            resp = client.post(url, json=payload, headers=headers)
            resp.raise_for_status()
            return resp.json()


class _AsyncHttpClient:
    """åŸºäºŽ httpx çš„å¼‚æ­¥ HTTP å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str, api_key: str):
        self.base_url = (base_url or "https://api.openai.com/v1").rstrip("/")
        self.api_key = api_key or "sk-placeholder"

    async def chat_completions_create(self, **kwargs: Any) -> Any:
        url = f"{self.base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": kwargs.get("model", "gpt-4o-mini"),
            "messages": kwargs.get("messages", []),
            "stream": kwargs.get("stream", False),
        }
        if "temperature" in kwargs:
            payload["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            payload["max_tokens"] = kwargs["max_tokens"]
        async with httpx.AsyncClient(timeout=60.0) as client:
            resp = await client.post(url, json=payload, headers=headers)
            resp.raise_for_status()
            return resp.json()

    async def chat_completions_create_stream(self, **kwargs: Any) -> AsyncIterator[str]:
        """æµå¼è°ƒç”¨ï¼Œé€å— yield æ–‡æœ¬å†…å®¹ï¼ˆOpenAI SSE æ ¼å¼ï¼‰ã€‚"""
        url = f"{self.base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": kwargs.get("model", "gpt-4o-mini"),
            "messages": kwargs.get("messages", []),
            "stream": True,
        }
        if "temperature" in kwargs:
            payload["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs:
            payload["max_tokens"] = kwargs["max_tokens"]
        async with httpx.AsyncClient(timeout=60.0) as client:
            async with client.stream("POST", url, json=payload, headers=headers) as resp:
                resp.raise_for_status()
                async for line in resp.aiter_lines():
                    if not line.startswith("data: "):
                        continue
                    data = line[6:].strip()
                    if data == "[DONE]":
                        return
                    try:
                        j = json.loads(data)
                        content = j.get("choices", [{}])[0].get("delta", {}).get("content") or ""
                        if content:
                            yield content
                    except (json.JSONDecodeError, KeyError, IndexError):
                        pass


async def chat_completion(
    messages: list[dict[str, str]],
    model: str | None = None,
    temperature: float = 0.7,
    max_tokens: int = 2048,
) -> str:
    """
    è°ƒç”¨ LLM å®Œæˆå¯¹è¯ï¼Œè¿”å›ž assistant çš„ content æ–‡æœ¬ã€‚
    """
    cfg = get_ai_config()
    model = model or cfg.model
    client = _create_async_client()
    try:
        if HAS_OPENAI:
            resp = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return (resp.choices[0].message.content or "").strip()
        # è‡ªå®šä¹‰ HttpClient
        resp = await client.chat_completions_create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )
        choices = resp.get("choices", [])
        if choices:
            content = choices[0].get("message", {}).get("content") or ""
            return content.strip()
        return ""
    except Exception as e:
        logger.error("LLM è°ƒç”¨å¤±è´¥: %s", e)
        raise


async def chat_completion_stream(
    messages: list[dict[str, str]],
    model: str | None = None,
    temperature: float = 0.7,
    max_tokens: int = 2048,
) -> AsyncIterator[str]:
    """
    æµå¼è°ƒç”¨ LLMï¼Œé€å— yield åŠ©æ‰‹å›žå¤çš„æ–‡æœ¬å†…å®¹ã€‚
    """
    cfg = get_ai_config()
    model = model or cfg.model
    client = _create_async_client()
    try:
        if HAS_OPENAI:
            stream = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
            )
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            return
        async for chunk in client.chat_completions_create_stream(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        ):
            yield chunk
    except Exception as e:
        logger.error("LLM æµå¼è°ƒç”¨å¤±è´¥: %s", e)
        raise


async def generate_push_content_with_llm(
    event_type: str,
    event_data: dict,
    raw_title: str,
    raw_description: str,
) -> tuple[str, str] | None:
    """
    ä½¿ç”¨ LLM ç»“åˆäº‹ä»¶æ•°æ®å¯¹æŽ¨é€å†…å®¹è¿›è¡Œä¸ªæ€§åŒ–æ¶¦è‰²ï¼Œä¿æŒåŽŸæœ‰æ ¼å¼ç»“æž„ä¸å˜ã€‚
    ç”¨äºŽåœ¨é…ç½®äº† push_personalize_with_llm æ—¶ï¼Œä»…æ¶¦è‰²æ­£æ–‡è¡¨è¿°ï¼Œæ ‡é¢˜ä¸Žæ ¼å¼å¸ƒå±€ä¸æ”¹å˜ã€‚

    Args:
        event_type: äº‹ä»¶ç±»åž‹ï¼Œå¦‚ weibo/huya/bilibili/checkin/weather ç­‰
        event_data: äº‹ä»¶æ•°æ®å­—å…¸ï¼ˆå¯ JSON åºåˆ—åŒ–ï¼‰ï¼ŒåŒ…å«é€šçŸ¥ç›¸å…³çš„åŽŸå§‹ä¿¡æ¯
        raw_title: åŽŸå§‹æ¨¡æ¿æ ‡é¢˜
        raw_description: åŽŸå§‹æ¨¡æ¿å†…å®¹

    Returns:
        (åŽŸæ ‡é¢˜, ä¸ªæ€§åŒ–åŽçš„å†…å®¹) å…ƒç»„ï¼Œæ ‡é¢˜ä¿æŒåŽŸæ ·ã€ä»…å†…å®¹è¢«æ¶¦è‰²ï¼›è‹¥ LLM è°ƒç”¨å¤±è´¥åˆ™è¿”å›ž None
    """
    import json

    # æž„å»ºç²¾ç®€çš„äº‹ä»¶æ‘˜è¦ï¼ˆé¿å… token çˆ†ç‚¸ï¼Œä»…ä¿ç•™å…³é”®å­—æ®µï¼‰
    try:
        summary = json.dumps(event_data, ensure_ascii=False, default=str)[:2000]
    except (TypeError, ValueError):
        summary = str(event_data)[:2000]

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŽ¨é€é€šçŸ¥åŠ©æ‰‹ã€‚è¯·åœ¨**ä¿æŒåŽŸæœ‰æ ¼å¼ç»“æž„ä¸å˜**çš„å‰æä¸‹ï¼Œå¯¹æŽ¨é€å†…å®¹è¿›è¡Œä¸ªæ€§åŒ–æ¶¦è‰²ã€‚

äº‹ä»¶ç±»åž‹ï¼š{event_type}
åŽŸå§‹å†…å®¹ï¼ˆæ ¼å¼ä¸Žç»“æž„è¯·åŽŸæ ·ä¿ç•™ï¼‰ï¼š{raw_description}

äº‹ä»¶æ•°æ®ï¼ˆJSON æˆ–æ‘˜è¦ï¼‰ï¼š{summary}

è¦æ±‚ï¼š
1. **æ ¼å¼ä¸å˜**ï¼šå¿…é¡»ä¿ç•™åŽŸæ–‡çš„å¸ƒå±€ã€åˆ†å—ï¼ˆå¦‚ã€ŒTaè¯´ã€ã€Œè®¤è¯ã€ã€Œç®€ä»‹ã€ã€Œæˆ¿é—´å·ã€ç­‰ï¼‰ã€åˆ†éš”ç¬¦å’Œå±‚çº§ç»“æž„
2. **å†…å®¹ä¸ªæ€§åŒ–**ï¼šåªå¯¹å†…å®¹è¿›è¡Œæ¶¦è‰²ï¼Œä½¿è¡¨è¿°æ›´è‡ªç„¶ã€è´´åˆ‡ï¼Œå¯é€‚åº¦åŠ å…¥äº²åˆ‡è¯­æ°”æˆ–ç®€è¦è§£è¯»ï¼Œä½†ä¸è¦æ”¹å˜å…³é”®æ•°æ®
3. ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦é—æ¼åŽŸå§‹å†…å®¹ä¸­çš„ä»»ä½•é‡è¦å­—æ®µ
4. ç›´æŽ¥è¾“å‡º JSONï¼Œæ ¼å¼ä¸¥æ ¼ä¸ºï¼š{{"description": "æ¶¦è‰²åŽçš„å†…å®¹"}}ï¼ˆåªè¾“å‡º description å­—æ®µï¼‰
5. ä¸è¦è¾“å‡ºå…¶ä»–è¯´æ˜Žæˆ–å¼•å·å¤–çš„å†…å®¹"""

    try:
        result = await chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=512,
        )
        if not result or not result.strip():
            return None
        # å°è¯•ä»Žç»“æžœä¸­æå– JSONï¼ˆå¯èƒ½è¢« markdown åŒ…è£¹ï¼‰
        text = result.strip()
        start = text.find("{")
        end = text.rfind("}") + 1
        if start >= 0 and end > start:
            j = json.loads(text[start:end])
            desc = j.get("description") or raw_description
            if isinstance(desc, str):
                # æ ‡é¢˜ä¿æŒä¸å˜ï¼Œä»…å†…å®¹ä¸ªæ€§åŒ–
                return (raw_title, desc.strip())
        return None
    except Exception as e:
        logger.warning("LLM ç”Ÿæˆä¸ªæ€§åŒ–æŽ¨é€å¤±è´¥ï¼Œå°†ä½¿ç”¨åŽŸå§‹æ¨¡æ¿: %s", e)
        return None


def _split_template_and_body(text: str) -> tuple[str, str, str] | None:
    """
    å°†æŽ¨é€å†…å®¹æ‹†åˆ†ä¸ºã€Œæ¨¡æ¿å‰ç¼€ + æ­£æ–‡ + æ¨¡æ¿åŽç¼€ã€ï¼Œä»…æ­£æ–‡éœ€è¦åŽ‹ç¼©æ—¶ä½¿ç”¨ã€‚
    è¯†åˆ«çº¦å®šæ ¼å¼ï¼ˆå¦‚å¾®åšçš„ Taè¯´:ðŸ‘‡\\næ­£æ–‡\\n====...\\nè®¤è¯/ç®€ä»‹ï¼‰ï¼Œä¿æŒæ¨¡æ¿ä¸å˜ã€‚

    Returns:
        (prefix, body, suffix) è‹¥åŒ¹é…åˆ°å·²çŸ¥æ¨¡æ¿æ ¼å¼ï¼Œå¦åˆ™ None
    """
    # å¾®åšæŽ¨é€æ ¼å¼ï¼šTaè¯´:ðŸ‘‡\\n{æ­£æ–‡}\\n=========================\\nè®¤è¯:...\\n\\nç®€ä»‹:...
    weibo_prefix = "Taè¯´:ðŸ‘‡\n"
    weibo_sep = "\n" + "=" * 25 + "\n"
    if text.startswith(weibo_prefix):
        rest = text[len(weibo_prefix) :]
        idx = rest.find(weibo_sep)
        if idx >= 0:
            return (weibo_prefix, rest[:idx], rest[idx:])
    return None


async def compress_text_with_llm(text: str, max_bytes: int) -> str | None:
    """
    ä½¿ç”¨ LLM å°†æ–‡æœ¬åŽ‹ç¼©åˆ°æŒ‡å®šå­—èŠ‚æ•°ä»¥å†…ï¼Œä¿ç•™æ ¸å¿ƒè¯­ä¹‰ã€‚
    è‹¥å†…å®¹ä¸ºå·²çŸ¥æ¨¡æ¿æ ¼å¼ï¼ˆå¦‚å¾®åš Taè¯´/è®¤è¯/ç®€ä»‹ï¼‰ï¼Œåˆ™ä»…åŽ‹ç¼©æ­£æ–‡éƒ¨åˆ†ï¼Œæ¨¡æ¿å‰ç¼€ä¸ŽåŽç¼€ä¿æŒä¸å˜ã€‚

    Args:
        text: åŽŸå§‹æ–‡æœ¬
        max_bytes: åŽ‹ç¼©åŽ UTF-8 ç¼–ç çš„æœ€å¤§å­—èŠ‚æ•°

    Returns:
        åŽ‹ç¼©åŽçš„æ–‡æœ¬ï¼Œè‹¥ LLM è°ƒç”¨å¤±è´¥æˆ–è¶…æ—¶åˆ™è¿”å›ž None
    """
    prefix, body, suffix = "", text, ""
    parts = _split_template_and_body(text)
    if parts:
        prefix, body, suffix = parts
        template_bytes = len((prefix + suffix).encode("utf-8"))
        if template_bytes >= max_bytes:
            # æ¨¡æ¿æœ¬èº«å·²è¶…é™ï¼Œå›žé€€ä¸ºæ•´æ®µåŽ‹ç¼©
            prefix, body, suffix = "", text, ""
        else:
            max_bytes = max_bytes - template_bytes

    # ç›®æ ‡å­—ç¬¦æ•°ï¼ˆä¸­æ–‡çº¦ 3 å­—èŠ‚/å­—ï¼Œé¢„ç•™ä½™é‡ï¼‰
    max_chars = max(max_bytes // 2, 20)

    prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬åŽ‹ç¼©ä¸ºç®€çŸ­æ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. åŽ‹ç¼©åŽçš„æ–‡å­—ä¸è¶…è¿‡ {max_chars} ä¸ªå­—ç¬¦ï¼ˆçº¦ {max_bytes} å­—èŠ‚ä»¥å†…ï¼‰
2. ä¿ç•™æ ¸å¿ƒè§‚ç‚¹ã€å…³é”®ä¿¡æ¯ï¼Œå¯çœç•¥å†—ä½™è¡¨è¿°
3. ä½¿ç”¨ç®€æ´è‡ªç„¶çš„æ±‰è¯­ï¼Œç›´æŽ¥è¾“å‡ºåŽ‹ç¼©åŽçš„æ–‡æœ¬ï¼Œä¸è¦åŠ å¼•å·æˆ–å¤šä½™è¯´æ˜Ž

åŽŸæ–‡ï¼š
{body}"""

    try:
        result = await chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=256,
        )
        if not result or not result.strip():
            return None
        result = result.strip()
        # æ ¡éªŒé•¿åº¦ï¼Œè‹¥ä»è¶…é™åˆ™è¿”å›ž Noneï¼ˆè°ƒç”¨æ–¹å°†å›žé€€åˆ°æˆªæ–­ï¼‰
        result_bytes = len(result.encode("utf-8"))
        if prefix or suffix:
            if result_bytes > max_bytes:
                logger.warning(
                    "LLM åŽ‹ç¼©æ­£æ–‡ä»è¶…é™ï¼ˆ%d > %d å­—èŠ‚ï¼‰ï¼Œå°†ä½¿ç”¨æˆªæ–­",
                    result_bytes,
                    max_bytes,
                )
                return None
            return prefix + result + suffix
        if result_bytes > max_bytes:
            logger.warning(
                "LLM åŽ‹ç¼©ç»“æžœä»è¶…é™ï¼ˆ%d > %d å­—èŠ‚ï¼‰ï¼Œå°†ä½¿ç”¨æˆªæ–­",
                result_bytes,
                max_bytes,
            )
            return None
        return result
    except Exception as e:
        logger.warning("LLM åŽ‹ç¼©æ–‡æœ¬å¤±è´¥ï¼Œå°†ä½¿ç”¨æˆªæ–­: %s", e)
        return None
